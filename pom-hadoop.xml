<?xml version="1.0" encoding="UTF-8"?>
<!--
  ~ Licensed to the Apache Software Foundation (ASF) under one
  ~ or more contributor license agreements.  See the NOTICE file
  ~ distributed with this work for additional information
  ~ regarding copyright ownership.  The ASF licenses this file
  ~ to you under the Apache License, Version 2.0 (the
  ~ "License"); you may not use this file except in compliance
  ~ with the License.  You may obtain a copy of the License at
  ~
  ~     http://www.apache.org/licenses/LICENSE-2.0
  ~
  ~ Unless required by applicable law or agreed to in writing, software
  ~ distributed under the License is distributed on an "AS IS" BASIS,
  ~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  ~ See the License for the specific language governing permissions and
  ~ limitations under the License.
  -->

<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
  <modelVersion>4.0.0</modelVersion>
  <groupId>org.apache</groupId>
  <artifactId>build-rpm</artifactId>
  <name>Build RPM Project</name>
  <packaging>pom</packaging>
  <version>1.0.0</version>
  <description>build rpm project</description>

  <properties>
    <!-- Set default encoding so multi-byte tests work correctly on the Mac -->
      <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
     <resources.dir>/source/BUILD-AMBARI/hadoop-resource</resources.dir>
<!--      <resources.dir>${project.basedir}/../hadoop-resource</resources.dir> -->
      <hadoop.name>hadoop</hadoop.name>
      <hadoop.version>3.1.1</hadoop.version>
      <hdp.version>0.2.0.0</hdp.version>
      <hdp.vname>0_2_0_0</hdp.vname>
      <hdp.release>02</hdp.release>
      <hadoop-dist.dir>/source/BUILD-AMBARI/hadoop/hadoop-dist</hadoop-dist.dir>
  <!--    <hadoop-source.dir>/source/BUILD-AMBARI/hadoop/hadoop-dist</hadoop-source.dir> -->
      <rpm.install.dir>/usr/odp/${hdp.version}-${hdp.release}</rpm.install.dir>
      <rpm.install.arch>x86_64</rpm.install.arch>
      <hadoop.jar.version>3.1.1</hadoop.jar.version>
  </properties>

  <modules>
<!--    <module>build-rpm-for-hadoop</module> -->
  </modules>

  <build>
      <plugins>
          <plugin>
            <groupId>org.codehaus.mojo</groupId>
            <artifactId>rpm-maven-plugin</artifactId>
            <version>2.1.5</version>
            <configuration>
				<!--prefix>/</prefix-->
                <copyright>2019, com.hw</copyright>
                <distribution>hw</distribution>
                <group>hw.com</group>
                <packager>hw</packager>
				<license>Apache License v2.0</license>
				<url>http://hadoop.apache.org/core/</url>
                <needarch>${rpm.install.arch}</needarch>
                <targetOS>linux</targetOS>
                <version>${hadoop.version}.${hdp.version}</version>
                <release>${hdp.release}</release>

				<defaultFilemode>644</defaultFilemode>
				<defaultDirmode>755</defaultDirmode>
				<defaultUsername>root</defaultUsername>
				<defaultGroupname>root</defaultGroupname>
            </configuration>
            <executions>
                <!--hadoop_3_1_0_0_1-->
                <execution>
                    <id>hadoop_${hdp.vname}_${hdp.release}</id>
                    <phase>package</phase>
                    <goals>
                        <goal>rpm</goal>
                    </goals>
                    <configuration>
                        <name>hadoop_${hdp.vname}_${hdp.release}</name>
                        <!--version>3.1.1.3.1.0.0</version-->
                        <description>
                            Hadoop is a software platform that lets one easily write and
                            run applications that process vast amounts of data.

                            Here's what makes Hadoop especially useful:
                            * Scalable: Hadoop can reliably store and process petabytes.
                            * Economical: It distributes the data and processing across clusters
                            of commonly available computers. These clusters can number
                            into the thousands of nodes.
                            * Efficient: By distributing the data, Hadoop can process it in parallel
                            on the nodes where the data is located. This makes it
                            extremely rapid.
                            * Reliable: Hadoop automatically maintains multiple copies of data and
                            automatically redeploys computing tasks based on failures.

                            Hadoop implements MapReduce, using the Hadoop Distributed File System (HDFS).
                            MapReduce divides applications into many small blocks of work. HDFS creates
                            multiple replicas of data blocks for reliability, placing them on compute
                            nodes around the cluster. MapReduce can then process the data where it is
                            located.
                        </description>
                        <summary>
                            Hadoop is a software platform for processing vast amounts of data
                        </summary>
                        <autoRequires>false</autoRequires>
                        <requires>
                            <require>zookeeper_${hdp.vname}_${hdp.release} >= 3.4.0</require>
                            <require>hdp-select >= ${hdp.version}-${hdp.release}</require>
                            <require>ranger_${hdp.vname}_${hdp.release}-hdfs-plugin</require>
                            <require>ranger_${hdp.vname}_${hdp.release}-yarn-plugin</require>
                            <require>spark2_${hdp.vname}_${hdp.release}-yarn-shuffle</require>
                        </requires>
                        <mappings>
                            <mapping>
                                <directory>${rpm.install.dir}/etc/bash_completion.d</directory>
                                <sources>
                                    <source>
<!--                                        <location>${hadoop-dist.dir}/target/hadoop-${hadoop.version}/bin/hadoop</location>  -->
                                        <location>${resources.dir}/etc/bash_completion.d</location>
                                    </source>
                                </sources>
                            </mapping>

                            <mapping>
                                <!--jars-->
                                <directory>${rpm.install.dir}/etc/hadoop/conf.empty</directory>
                                <sources>
                                    <source>
                                      <location>${hadoop-dist.dir}/target/hadoop-${hadoop.version}/etc/hadoop</location>
<!--                                      <location>${resources.dir}/etc/hadoop/conf.empty</location> -->
                                        <includes>
                                            <include>capacity-scheduler.xml</include>
                                            <include>configuration.xsl</include>
                                            <include>container-executor.cfg</include>
                                            <include>core-site.xml</include>
                                            <include>hadoop-env.cmd</include>
                                            <include>hadoop-env.sh</include>
                                            <include>hadoop-metrics2.properties</include>
                                            <include>hadoop-policy.xml</include>
                                            <include>kms-acls.xml</include>
                                            <include>kms-env.sh</include>
                                            <include>kms-log4j.properties</include>
                                            <include>kms-site.xml</include>
                                            <include>log4j.properties</include>
                                            <include>mapred-env.cmd</include>
                                            <include>mapred-env.sh</include>
                                            <include>mapred-queues.xml.template</include>
                                            <include>ssl-client.xml.example</include>
                                            <include>ssl-server.xml.example</include>
                                            <include>user_ec_policies.xml.template</include>
                                            <include>workers</include>
                                            <include>yarn-env.cmd</include>
                                            <include>yarnservice-log4j.properties</include>
                                        </includes>
                                    </source>
                                </sources>
                            </mapping>
                            <mapping>
                                <directory>${rpm.install.dir}/hadoop</directory>
                                <sources>
                                    <source>
                                        <location>${hadoop-dist.dir}/target/hadoop-${hadoop.version}/share/hadoop/tools/lib/</location>
                                        <includes>
                                            <include>azure*.jar</include>
                                            <include>hadoop-azure*.jar</include>
                                        </includes>
                                    </source>
                                    <source>
                                        <location>${hadoop-dist.dir}/target/hadoop-${hadoop.version}/share/hadoop/common/lib/</location>
                                        <includes>
                                            <include>hadoop-annotations-*.jar</include>
                                            <include>hadoop-auth-*.jar</include>
                                        </includes>
                                    </source>
                                    <source>
                                        <location>${hadoop-dist.dir}/target/hadoop-${hadoop.version}/share/hadoop/common/</location>
                                        <includes>
                                            <include>hadoop-common-*.jar</include>
                                            <include>hadoop-kms-*.jar</include>
                                            <include>hadoop-nfs-*.jar</include>
                                        </includes>
                                    </source>
                                    <source>
                                        <location>${hadoop-dist.dir}/target</location>
                                        <includes>
                                            <include>hadoop-*.tar.gz</include>
                                        </includes>
                                    </source>
                                </sources>
                            </mapping>
                            <mapping>
                                <directory>${rpm.install.dir}/hadoop/bin</directory>
                                <sources>
                                    <source>
                                        <location>${hadoop-dist.dir}/target/hadoop-${hadoop.version}/bin</location>
                                        <includes>
                                            <include>hadoop</include>
                                            <include>hdfs</include>
                                            <!--include>hadoop-fuse-dfs</include-->
                                            <!--include>hadoop.distro</include-->
                                            <include>kill-data-node</include>
                                            <include>kill-name-node</include>
                                            <include>kill-secondary-name-node</include>
                                            <include>mapred</include>
                                            <include>yarn</include>
                                        </includes>
                                    </source>
                                   <source>
                                        <location>${resources.dir}</location>
                                        <includes>
                                            <include>hadoop-fuse-dfs</include>
                                            <include>hadoop.distro</include>
                                        </includes>
                                    </source>
                                </sources>
                            </mapping>
                            <mapping>
                                <directory>${rpm.install.dir}/hadoop/etc</directory>
                            </mapping>
                            <mapping>
                                <directory>${rpm.install.dir}/hadoop/lib</directory>
                                <sources>
                                    <source>
                                        <location>${hadoop-dist.dir}/target/hadoop-${hadoop.version}/share/hadoop/common/lib</location>
                                        <excludes>
                                            <exclude>hadoop-*.jar</exclude>
                                        </excludes>
                                    </source>
                                </sources>
                            </mapping>
                            <mapping>
                                <directory>${rpm.install.dir}/hadoop/lib/native</directory>
                                <sources>
                                    <source>
                                        <location>${hadoop-dist.dir}/target/hadoop-${hadoop.version}/lib/native/</location>
                                        <includes>
                                            <include>libhadoop.a</include>
                                            <include>libhadoop.so.1.0.0</include>
                                            <include>libhadooppipens.a</include>
                                            <include>libhadooputils.a</include>
                                            <include>libhdfs.a</include>
                                            <include>libnativetask.a</include>
                                            <include>libsnappy.so.1.1.4</include>
                                        </includes>
                                    </source>
<!--                                    <source>
                                        <location>${resources.dir}</location>
                                        <includes>
                                            <include>libsnappy.so.1.1.4</include>
                                        </includes>
                                    </source>
-->
                                </sources>
                            </mapping>
                            <mapping>
                                <directory>${rpm.install.dir}/hadoop/libexec</directory>
                                <sources>
                                    <source>
                                        <location>${hadoop-dist.dir}/target/hadoop-${hadoop.version}/libexec</location>
                                        <includes>
                                            <include>hadoop-config.cmd</include>
                                            <include>hadoop-config.sh</include>
                                            <include>hadoop-functions.sh</include>
                                            <include>hadoop-layout.sh.example</include>
                                            <include>hadoop-layout.sh</include>
                                            <include>hdfs-config.cmd</include>
                                            <include>mapred-config.cmd</include>
                                            <include>yarn-config.cmd</include>
                                        </includes>
                                    </source>
                                </sources>
                            </mapping>
                            <mapping>
                                <directory>${rpm.install.dir}/hadoop/libexec/shellprofile.d/</directory>
                                <sources>
                                    <source>
                                        <location>${hadoop-dist.dir}/target/hadoop-${hadoop.version}/libexec/shellprofile.d/</location>
                                    </source>
                                </sources>
                            </mapping>
                            <mapping>
                                <directory>${rpm.install.dir}/hadoop/man/man1</directory>
                                <sources>
                                    <source>
                                        <location>${resources.dir}</location>
                                        <includes>
                                            <include>hadoop.1.gz</include>
                                        </includes>
                                    </source>
                                </sources>
                            </mapping>
                            <mapping>
                                <directory>${rpm.install.dir}/hadoop/sbin</directory>
                                <sources>
                                    <source>
                                        <location>${hadoop-dist.dir}/target/hadoop-${hadoop.version}/sbin</location>
                                        <includes>
                                            <include>hadoop-daemon.sh</include>
                                            <include>hadoop-daemons.sh</include>
                                            <include>workers.sh</include>
                                        </includes>
                                    </source>
                                </sources>
                            </mapping>
                            <mapping>
                                <directory>${rpm.install.dir}/../current</directory>
                            </mapping>
                        </mappings>
                        <preinstallScriptlet>
                            <script>
                                <!-- 清理 -->
                                echo "installing now"
                            </script>
                        </preinstallScriptlet>
                        <!-- rpm安装后执行的脚本-->
                        <postinstallScriptlet>
                            <script>
                                <!-- 处理mapreduce.tar.gz -->
                                cd ${rpm.install.dir}/hadoop
                                tar xf hadoop-${hadoop.version}.tar.gz
                                mv hadoop-${hadoop.version} hadoop
                                tar zcf mapreduce.tar.gz hadoop
                                rm -rf hadoop hadoop-${hadoop.version}

                                cd ${rpm.install.dir}/hadoop/bin
                                sed -i "s/0.2.0.0-02/${hdp.version}-${hdp.release}/g" hadoop-fuse-dfs

                                rm -rf /etc/hadoop/conf
                                mkdir -p /etc/hadoop/conf
                                cp -r ${rpm.install.dir}/etc/hadoop/conf.empty/* /etc/hadoop/conf/
                                cd ${rpm.install.dir}/hadoop/
                                ln -s /etc/hadoop/conf conf
                                cd ${rpm.install.dir}/hadoop/etc/
                                ln -s ../../hadoop/conf hadoop
                                cd ${rpm.install.dir}/hadoop/
                                ln -s hadoop-annotations-${hadoop.version}.jar hadoop-annotations.jar
                                ln -s hadoop-auth-${hadoop.version}.jar hadoop-auth.jar
                                ln -s hadoop-azure-datalake-${hadoop.version}.jar hadoop-azure-datalake.jar
                                ln -s hadoop-azure-${hadoop.version}.jar hadoop-azure.jar
                                ln -s hadoop-common-${hadoop.version}-tests.jar hadoop-common-tests.jar
                                ln -s hadoop-common-${hadoop.version}.jar hadoop-common.jar
                                ln -s hadoop-kms-${hadoop.version}.jar hadoop-kms.jar
                                ln -s hadoop-nfs-${hadoop.version}.jar hadoop-nfs.jar
                                cd ${rpm.install.dir}/hadoop/lib/native/
                                ln -s libhadoop.so.1.0.0 libhadoop.so
                                ln -s libsnappy.so.1.1.4 libsnappy.so
                                ln -s libsnappy.so.1.1.4 libsnappy.so.1

                                cd ${rpm.install.dir}/../current
                                rm -rf hadoop-client hadoop-hdfs-client hadoop-hdfs-datanode hadoop-hdfs-journalnode hadoop-hdfs-namenode hadoop-hdfs-nfs3 hadoop-hdfs-portmap hadoop-hdfs-secondarynamenode
                                rm -rf hadoop-hdfs-zkfc hadoop-httpfs hadoop-mapreduce-client hadoop-mapreduce-historyserver hadoop-yarn-client hadoop-yarn-nodemanager hadoop-yarn-registrydns
                                rm -rf hadoop-yarn-resourcemanager hadoop-yarn-timelinereader hadoop-yarn-timelineserver hive-client livy2-client livy-client pig-client shc spark2-client
                                rm -rf spark-client spark_llap spark-schema-registry tez-client

                                ln -s ${rpm.install.dir}/hadoop ${rpm.install.dir}/../current/hadoop-client
                                ln -s ${rpm.install.dir}/hadoop-hdfs ${rpm.install.dir}/../current/hadoop-hdfs-client
                                ln -s ${rpm.install.dir}/hadoop-hdfs ${rpm.install.dir}/../current/hadoop-hdfs-datanode
                                ln -s ${rpm.install.dir}/hadoop-hdfs ${rpm.install.dir}/../current/hadoop-hdfs-journalnode
                                ln -s ${rpm.install.dir}/hadoop-hdfs ${rpm.install.dir}/../current/hadoop-hdfs-namenode
                                ln -s ${rpm.install.dir}/hadoop-hdfs ${rpm.install.dir}/../current/hadoop-hdfs-nfs3
                                ln -s ${rpm.install.dir}/hadoop-hdfs ${rpm.install.dir}/../current/hadoop-hdfs-portmap
                                ln -s ${rpm.install.dir}/hadoop-hdfs ${rpm.install.dir}/../current/hadoop-hdfs-secondarynamenode
                                ln -s ${rpm.install.dir}/hadoop-hdfs ${rpm.install.dir}/../current/hadoop-hdfs-zkfc
                                ln -s ${rpm.install.dir}/hadoop-httpfs ${rpm.install.dir}/../current/hadoop-httpfs
                                ln -s ${rpm.install.dir}/hadoop-mapreduce ${rpm.install.dir}/../current/hadoop-mapreduce-client
                                ln -s ${rpm.install.dir}/hadoop-mapreduce ${rpm.install.dir}/../current/hadoop-mapreduce-historyserver
                                ln -s ${rpm.install.dir}/hadoop-yarn ${rpm.install.dir}/../current/hadoop-yarn-client
                                ln -s ${rpm.install.dir}/hadoop-yarn ${rpm.install.dir}/../current/hadoop-yarn-nodemanager
                                ln -s ${rpm.install.dir}/hadoop-yarn ${rpm.install.dir}/../current/hadoop-yarn-registrydns
                                ln -s ${rpm.install.dir}/hadoop-yarn ${rpm.install.dir}/../current/hadoop-yarn-resourcemanager
                                ln -s ${rpm.install.dir}/hadoop-yarn ${rpm.install.dir}/../current/hadoop-yarn-timelinereader
                                ln -s ${rpm.install.dir}/hadoop-yarn ${rpm.install.dir}/../current/hadoop-yarn-timelineserver
                                ln -s ${rpm.install.dir}/hive ${rpm.install.dir}/../current/hive-client
                                ln -s ${rpm.install.dir}/livy2 ${rpm.install.dir}/../current/livy2-client
                                ln -s ${rpm.install.dir}/livy ${rpm.install.dir}/../current/livy-client
                                ln -s ${rpm.install.dir}/pig ${rpm.install.dir}/../current/pig-client
                                ln -s ${rpm.install.dir}/shc ${rpm.install.dir}/../current/shc
                                ln -s ${rpm.install.dir}/spark2 ${rpm.install.dir}/../current/spark2-client
                                ln -s ${rpm.install.dir}/spark ${rpm.install.dir}/../current/spark-client
                                ln -s ${rpm.install.dir}/spark_llap ${rpm.install.dir}/../current/spark_llap
                                ln -s ${rpm.install.dir}/spark-schema-registry ${rpm.install.dir}/../current/spark-schema-registry
                                ln -s ${rpm.install.dir}/tez ${rpm.install.dir}/../current/tez-client
								ln -s ${rpm.install.dir}/sqoop ${rpm.install.dir}/../current/sqoop-client
								ln -s ${rpm.install.dir}/sqoop ${rpm.install.dir}/../current/sqoop-server




                                <!-- /usr/bin 下软连接-->
                                rm -rf /usr/bin/beeline /usr/bin/hadoop /usr/bin/hcat /usr/bin/hdfs /usr/bin/hive /usr/bin/mapred  /usr/bin/yarn /usr/bin/pig
                                ln -s ${rpm.install.dir}/../current/hive-client/bin/beeline /usr/bin/beeline
                                ln -s ${rpm.install.dir}/../current/hive-client/../hive-hcatalog/bin/hcat /usr/bin/hcat
                                ln -s ${rpm.install.dir}/../current/hadoop-hdfs-client/bin/hdfs /usr/bin/hdfs
                                ln -s ${rpm.install.dir}/../current/hive-client/bin/hive /usr/bin/hive
                                ln -s ${rpm.install.dir}/../current/hadoop-mapreduce-client/bin/mapred /usr/bin/mapred
                                ln -s ${rpm.install.dir}/../current/hadoop-client/bin/hadoop /usr/bin/hadoop
                                ln -s ${rpm.install.dir}/../current/hadoop-yarn-client/bin/yarn /usr/bin/yarn
                                ln -s ${rpm.install.dir}/../current/pig-client/bin/pig /usr/bin/pig

                                <!-- /bin 下软连接-->
                                rm -rf /bin/beeline /bin/hadoop /bin/hcat /bin/hdfs /bin/hive /bin/mapred  /bin/yarn /bin/pig
                                ln -s ${rpm.install.dir}/../current/hive-client/bin/beeline /bin/beeline
                                ln -s ${rpm.install.dir}/../current/hive-client/../hive-hcatalog/bin/hcat /bin/hcat
                                ln -s ${rpm.install.dir}/../current/hadoop-hdfs-client/bin/hdfs /bin/hdfs
                                ln -s ${rpm.install.dir}/../current/hive-client/bin/hive /bin/hive
                                ln -s ${rpm.install.dir}/../current/hadoop-mapreduce-client/bin/mapred /bin/mapred
                                ln -s ${rpm.install.dir}/../current/hadoop-client/bin/hadoop /bin/hadoop
                                ln -s ${rpm.install.dir}/../current/hadoop-yarn-client/bin/yarn /bin/yarn
                                ln -s ${rpm.install.dir}/../current/pig-client/bin/pig /bin/pig

                                find ${rpm.install.dir} -name *.sh  | xargs chmod +x ;
                                echo "install success!";
                            </script>
                        </postinstallScriptlet>
                        <preremoveScriptlet>
                            <script>
                                <!-- rm -rf /home/installed/hadoop ; -->
                                cd ${rpm.install.dir}/hadoop/
                                rm -rf conf mapreduce.tar.gz hadoop hadoop-${hadoop.version}

                                cd ${rpm.install.dir}/hadoop/etc/
                                rm -rf hadoop
                                cd ${rpm.install.dir}/hadoop/
                                rm -rf hadoop-annotations.jar
                                rm -rf hadoop-auth.jar
                                rm -rf hadoop-azure-datalake.jar
                                rm -rf hadoop-azure.jar
                                rm -rf hadoop-common-tests.jar
                                rm -rf hadoop-common.jar
                                rm -rf hadoop-kms.jar
                                rm -rf hadoop-nfs.jar
                                cd ${rpm.install.dir}/hadoop/lib/native/
                                rm -rf libhadoop.so
                                rm -rf libsnappy.so libsnappy.so.1

                                echo "uninstalling  success";
                            </script>
                            <!-- 引用脚本方式
                                                                              <scriptFile>src/main/scripts/preremove</scriptFile>
                            <fileEncoding>utf-8</fileEncoding>
                            -->
                        </preremoveScriptlet>
                    </configuration>
                </execution>
            </executions>
          </plugin>
        </plugins>
      </build>
</project>
