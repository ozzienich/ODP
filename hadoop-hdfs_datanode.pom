<?xml version="1.0" encoding="UTF-8"?>
<!--
  ~ Licensed to the Apache Software Foundation (ASF) under one
  ~ or more contributor license agreements.  See the NOTICE file
  ~ distributed with this work for additional information
  ~ regarding copyright ownership.  The ASF licenses this file
  ~ to you under the Apache License, Version 2.0 (the
  ~ "License"); you may not use this file except in compliance
  ~ with the License.  You may obtain a copy of the License at
  ~
  ~     http://www.apache.org/licenses/LICENSE-2.0
  ~
  ~ Unless required by applicable law or agreed to in writing, software
  ~ distributed under the License is distributed on an "AS IS" BASIS,
  ~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  ~ See the License for the specific language governing permissions and
  ~ limitations under the License.
  -->

<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
  <modelVersion>4.0.0</modelVersion>
  <groupId>org.apache</groupId>
  <artifactId>build-rpm</artifactId>
  <name>Build RPM Project</name>
  <packaging>pom</packaging>
  <version>1.0.0</version>
  <description>build rpm project</description>

  <properties>
    <!-- Set default encoding so multi-byte tests work correctly on the Mac -->
      <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
     <resources.dir>/source/BUILD-ODP/hadoop-resource/HDFS-DATANODE</resources.dir>
<!--      <resources.dir>${project.basedir}/../hadoop-resource</resources.dir> -->
      <hadoop.name>hadoop</hadoop.name>
      <hadoop.subname>hdfs-datanode</hadoop.subname>
      <hadoop.version>3.1.1</hadoop.version>
      <hdp.version>0.2.0.0</hdp.version>
      <hdp.vname>0_2_0_0</hdp.vname>
      <hdp.release>02</hdp.release>
      <hadoop-dist.dir>/source/BUILD-ODP/hadoop/hadoop-dist</hadoop-dist.dir>
  <!--    <hadoop-source.dir>/source/BUILD-ODP/hadoop/hadoop-dist</hadoop-source.dir> -->
      <rpm.install.dir>/usr/odp/${hdp.version}-${hdp.release}</rpm.install.dir>
      <rpm.install.arch>x86_64</rpm.install.arch>
      <hadoop.jar.version>3.1.1</hadoop.jar.version>
  </properties>

  <modules>
<!--    <module>build-rpm-for-hadoop</module> -->
  </modules>

  <build>
      <plugins>
          <plugin>
            <groupId>org.codehaus.mojo</groupId>
            <artifactId>rpm-maven-plugin</artifactId>
            <version>2.1.5</version>
            <configuration>
				<!--prefix>/</prefix-->
                <copyright>2021, optimasidata.com</copyright>
                <distribution>ODP</distribution>
                <group>ODP</group>
                <packager>ozzie</packager>
                <license>Apache License v2.0</license>
                <url>https://www.optimasidata.com/ODP/</url>
                <needarch>${rpm.install.arch}</needarch>
                <targetOS>linux</targetOS>
<!--                <version>${hadoop.version}.${hdp.version}</version>  -->
                <version>${hadoop.version}</version>
                <release>centos7</release>

				<defaultFilemode>644</defaultFilemode>
				<defaultDirmode>755</defaultDirmode>
				<defaultUsername>root</defaultUsername>
				<defaultGroupname>root</defaultGroupname>
            </configuration>
            <executions>
                <!--hadoop_3_1_0_0_1-->
                <execution>
                    <id>hadoop_${hdp.vname}_${hdp.release}-hdfs-datanode</id>
                    <phase>package</phase>
                    <goals>
                        <goal>rpm</goal>
                    </goals>
                    <configuration>
                        <name>hadoop_${hdp.vname}_${hdp.release}-hdfs-datanode</name>
                        <!--version>3.1.1.3.1.0.0</version-->
                        <description>
                            Hadoop is a software platform that lets one easily write and
                            run applications that process vast amounts of data.

                            Here's what makes Hadoop especially useful:
                            * Scalable: Hadoop can reliably store and process petabytes.
                            * Economical: It distributes the data and processing across clusters
                            of commonly available computers. These clusters can number
                            into the thousands of nodes.
                            * Efficient: By distributing the data, Hadoop can process it in parallel
                            on the nodes where the data is located. This makes it
                            extremely rapid.
                            * Reliable: Hadoop automatically maintains multiple copies of data and
                            automatically redeploys computing tasks based on failures.

                            Hadoop implements MapReduce, using the Hadoop Distributed File System (HDFS).
                            MapReduce divides applications into many small blocks of work. HDFS creates
                            multiple replicas of data blocks for reliability, placing them on compute
                            nodes around the cluster. MapReduce can then process the data where it is
                            located.
                        </description>
                        <summary>
                            Hadoop is a software platform for processing vast amounts of data
                        </summary>
                        <autoRequires>false</autoRequires>
                        <mappings>
                            <mapping>
                                <directory>${rpm.install.dir}/etc/default</directory>
                                <sources>
                                    <source>
<!--                                        <location>${hadoop-dist.dir}/target/hadoop-${hadoop.version}/bin/hadoop</location>  -->
                                        <location>${resources.dir}/etc/default</location>
                                    </source>
                                </sources>
                            </mapping>

                            <mapping>
                                <!--jars-->
                                <directory>${rpm.install.dir}/hadoop-hdfs</directory>
                                <sources>
                                    <source>
                                      <location>${resources.dir}/hadoop-hdfs</location>
<!--                                      <location>${resources.dir}/etc/hadoop/conf.empty</location> -->
                                    </source>
                                </sources>
                            </mapping>
                        </mappings>
                        <preinstallScriptlet>
                            <script>
                                <!-- 清理 -->
                                echo "installing now"

                            </script>
                        </preinstallScriptlet>

                        <postinstallScriptlet>
                            <script>
                              cd ${rpm.install.dir}/hadoop-hdfs/etc/rc.d/init.d
                              chmod +x hadoop-hdfs-datanode
                            </script>
                        </postinstallScriptlet>
                        <script>
                        echo "saddsadas"
                        </script>
                        <preremoveScriptlet>
                          <script>
                            cd ${rpm.install.dir}/hadoop-hdfs/etc/rc.d/init.d
                            rm -rf hadoop-hdfs-datanode
                          </script>
                        </preremoveScriptlet>
                    </configuration>
                </execution>
            </executions>
          </plugin>
        </plugins>
      </build>
</project>
